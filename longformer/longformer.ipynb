{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/d0/5f/f41b14a398d484bf218d5167ec9061c1e76f500d9e25166117818c8bacda/torch-2.3.1-cp311-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torch-2.3.1-cp311-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /Users/tainejarvis/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Obtaining dependency information for typing-extensions>=4.8.0 from https://files.pythonhosted.org/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /Users/tainejarvis/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/tainejarvis/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/tainejarvis/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/tainejarvis/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tainejarvis/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/tainejarvis/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.3.1-cp311-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "Successfully installed torch-2.3.1 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LongformerSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, attention_window, global_tokens):\n",
    "        \"\"\"\n",
    "        embed_dim: Dimensionality of the input embeddings\n",
    "        num_heads: Number of attention heads\n",
    "        attention_window: Size of the local attention window\n",
    "        global_tokens: Indices of tokens that will have global attention\n",
    "        \"\"\"\n",
    "        super(LongformerSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_window = attention_window\n",
    "        self.global_tokens = global_tokens\n",
    "\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads.\"\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_projection = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        # Linear transformations\n",
    "        q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Sliding window local attention\n",
    "        local_attention_scores = self.local_attention(q, k)\n",
    "        \n",
    "        # Compute global attention scores\n",
    "        global_attention_scores = self.global_attention(q, k)\n",
    "        \n",
    "        # Combine local and global attention scores\n",
    "        attention_scores = local_attention_scores + global_attention_scores\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Compute context vectors\n",
    "        context = torch.matmul(attention_weights, v)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
    "        \n",
    "        # Final linear transformation\n",
    "        output = self.out_projection(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def local_attention(self, q, k):\n",
    "        batch_size, num_heads, seq_len, head_dim = q.size()\n",
    "        window = self.attention_window\n",
    "        \n",
    "        attn_mask = self.sliding_window_mask(seq_len, window)  # Create local mask\n",
    "        attn_mask = attn_mask.to(q.device)                      # Move to device\n",
    "        \n",
    "        attention_scores = torch.einsum(\"bnqd,bnkd->bnqk\", q, k) / (head_dim ** 0.5)\n",
    "        attention_scores.masked_fill_(attn_mask, float('-inf')) # Apply local mask\n",
    "        \n",
    "        return attention_scores\n",
    "\n",
    "    def global_attention(self, q, k):\n",
    "        batch_size, num_heads, seq_len, head_dim = q.size()\n",
    "        global_mask = torch.zeros((seq_len, seq_len), dtype=torch.bool)\n",
    "        \n",
    "        for token in self.global_tokens:\n",
    "            global_mask[token, :] = 1\n",
    "        \n",
    "        global_mask = global_mask.unsqueeze(0).unsqueeze(0)  # For batch and heads\n",
    "        global_mask = global_mask.to(q.device)               # Move to device\n",
    "        \n",
    "        global_attention_scores = torch.einsum(\"bnqd,bnkd->bnqk\", q, k) / (head_dim ** 0.5)\n",
    "        global_attention_scores.masked_fill_(~global_mask, float('-inf')) # Apply global mask\n",
    "        \n",
    "        return global_attention_scores\n",
    "    \n",
    "    def sliding_window_mask(self, seq_len, window):\n",
    "        mask = torch.ones((seq_len, seq_len), dtype=torch.bool)\n",
    "        for i in range(seq_len):\n",
    "            mask[i, max(0, i-window):min(seq_len, i+window+1)] = 0\n",
    "        return mask.unsqueeze(0).unsqueeze(0) # For batch and heads\n",
    "\n",
    "# Example usage\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "attention_window = 5\n",
    "global_tokens = [0, 1, 2]\n",
    "\n",
    "x = torch.rand((2, 20, embed_dim))  # Batch size: 2, Sequence length: 20, Embedding dimension: 256\n",
    "attention_layer = LongformerSelfAttention(embed_dim, num_heads, attention_window, global_tokens)\n",
    "output = attention_layer(x)\n",
    "print(output.shape)  # Should be torch.Size([2, 20, 256])\n",
    "                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
